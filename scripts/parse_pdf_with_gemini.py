#!/usr/bin/env python3
"""
Script to parse quiz pack PDF files using Gemini API and convert to JSON format
Usage: python parse_pdf_with_gemini.py <pdf_file_path>
"""

import os
import sys
import json
import time
from google import genai
from google.genai import types
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    print("Error: GEMINI_API_KEY environment variable not set")
    print("Please set it in .env file or with: export GEMINI_API_KEY='your-api-key'")
    sys.exit(1)

client = genai.Client(api_key=GEMINI_API_KEY)

SYSTEM_PROMPT = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–∞—Ä—Å–∏–Ω–≥—É PDF —Ñ–∞–π–ª–æ–≤ —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è –∏–≥—Ä—ã "–°–≤–æ—è –∏–≥—Ä–∞" –≤ —Ñ–æ—Ä–º–∞—Ç JSON.

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û! –õ–û–ì–ò–ö–ê –ü–û–õ–Ø "FORM":
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚ö†Ô∏è –ü–û–õ–ï "form" ‚â† –û–¢–í–ï–¢! ‚ö†Ô∏è

"form" - —ç—Ç–æ –º–∞—Ä–∫–µ—Ä/—É–∫–∞–∑–∞–Ω–∏–µ –ß–¢–û –∏–º–µ–Ω–Ω–æ –Ω—É–∂–Ω–æ –Ω–∞–∑–≤–∞—Ç—å, –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –ò–ó –°–ê–ú–û–ì–û –í–û–ü–†–û–°–ê.

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
–ê–õ–ì–û–†–ò–¢–ú –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø "FORM":
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

1Ô∏è‚É£ –ò–©–ï–ú –ó–ê–ì–õ–ê–í–ù–´–ï –°–õ–û–í–ê/–ú–ï–°–¢–û–ò–ú–ï–ù–ò–Ø –í –í–û–ü–†–û–°–ï:
   - "–í –ß–ï–°–¢–¨ –ù–ï–ì–û –Ω–∞–∑–≤–∞–ª–∏ —è—â–µ—Ä–∏—Ü—É" ‚Üí form: "–≤ —á–µ—Å—Ç—å –Ω–µ–≥–æ"
   - "–≠–¢–û–ì–û –ì–û–õ–õ–ê–ù–î–¶–ê —É–ø–æ–º–∏–Ω–∞—é—Ç" ‚Üí form: "–≥–æ–ª–ª–∞–Ω–¥–µ—Ü" / "—ç—Ç–æ–≥–æ –≥–æ–ª–ª–∞–Ω–¥—Ü–∞"
   - "–û–ù —à—É—Ç–∏–ª, —á—Ç–æ —Ö–æ—Ç–µ–ª" ‚Üí form: "–æ–Ω"
   - "–ï–ì–û —Ö–æ—Ç–µ–ª–∏ —Å–æ–∑–¥–∞—Ç—å" ‚Üí form: "–µ–≥–æ"
   - "–û–ù–ò –ø–æ–¥–≤–µ—Ä–≥–∞–ª–∏—Å—å –≥–æ–Ω–µ–Ω–∏—è–º" ‚Üí form: "–æ–Ω–∏"
   - "–° –ù–ï–ô —Å–≤—è–∑–∞–Ω–æ –º–Ω–æ–≥–æ –ª–µ–≥–µ–Ω–¥" ‚Üí form: "–æ–Ω–∞"
   - "–ò–ú–ò –æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –¥–µ–≤—É—à–∫–∏" ‚Üí form: "–∏–º–∏"
   - "–¢–ê–ö–ò–ï –û–ù–ò –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –∏–≥—Ä–µ" ‚Üí form: "—Ç–∞–∫–∏–µ –æ–Ω–∏"
   - "–≠–¢–û–ô –°–¢–û–õ–ò–¶–ï–ô —è–≤–ª—è–µ—Ç—Å—è" ‚Üí form: "—Å—Ç–æ–ª–∏—Ü–∞" / "—ç—Ç–∞ —Å—Ç–æ–ª–∏—Ü–∞"
   - "–≠–¢–û–¢ –ì–û–†–û–î —Å—Ç–∞–ª –ø–æ–±—Ä–∞—Ç–∏–º–æ–º" ‚Üí form: "—ç—Ç–æ—Ç –≥–æ—Ä–æ–¥" / "–≥–æ—Ä–æ–¥"
   - "–í –≠–¢–û–ú –ì–û–î–£ –ø–æ–≥–∏–±–ª–∞ –ò–¥–∞" ‚Üí form: "–≥–æ–¥"
   - "–≠–¢–û–ì–û –ü–†–û–î–Æ–°–ï–†–ê —Ä–∞–±–æ—Ç–æ–π –±—ã–ª–æ" ‚Üí form: "–ø—Ä–æ–¥—é—Å–µ—Ä"

2Ô∏è‚É£ –î–û–ë–ê–í–õ–Ø–ï–ú –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø –ò–ó –ù–ê–ß–ê–õ–ê –í–û–ü–†–û–°–ê:
   - "–í –æ—Ç–≤–µ—Ç–µ –æ–¥–Ω–æ —Å–ª–æ–≤–æ. –û–ù–ê –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è" ‚Üí form: "–æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º, –æ–Ω–∞"
   - "–í –æ—Ç–≤–µ—Ç–µ –¥–≤–∞ —Å–ª–æ–≤–∞. –û–ù –∏–∑–≤–µ—Å—Ç–µ–Ω" ‚Üí form: "–¥–≤–∞ —Å–ª–æ–≤–∞, –æ–Ω"
   - "–í –æ—Ç–≤–µ—Ç–µ —Ç—Ä–∏ —Å–ª–æ–≤–∞. –ï–ì–û –Ω–∞–∑—ã–≤–∞—é—Ç" ‚Üí form: "—Ç—Ä–µ–º—è —Å–ª–æ–≤–∞–º–∏, –µ–≥–æ"
   - "–í –æ—Ç–≤–µ—Ç–µ —Ç—Ä—ë—Ö—Å–ª–æ–∂–Ω–æ–µ —Å–ª–æ–≤–æ. –û–ù–û" ‚Üí form: "—Ç—Ä—ë—Ö—Å–ª–æ–∂–Ω–æ–µ —Å–ª–æ–≤–æ, –æ–Ω–æ"
   - "–í –æ—Ç–≤–µ—Ç–µ –¥–≤–∞ —Å–ª–æ–≤–∞ –Ω–∞ –æ–¥–Ω—É –±—É–∫–≤—É" ‚Üí form: "–¥–≤–∞ —Å–ª–æ–≤–∞ –Ω–∞ –æ–¥–Ω—É –±—É–∫–≤—É, –∏–º"
   - "–í –æ—Ç–≤–µ—Ç–µ –¥–≤–∞ —Å–ª–æ–≤–∞ –Ω–∞ –ø–∞—Ä–Ω—ã–µ —Å–æ–≥–ª–∞—Å–Ω—ã–µ" ‚Üí form: "–¥–≤–∞ —Å–ª–æ–≤–∞ –Ω–∞ –ø–∞—Ä–Ω—ã–µ —Å–æ–≥–ª–∞—Å–Ω—ã–µ"
   - "–ó–∞—á–µ—Ç –∞–±—Å–æ–ª—é—Ç–Ω–æ —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç" ‚Üí –¥–æ–±–∞–≤—å —ç—Ç–æ –≤ form

3Ô∏è‚É£ –ò–°–ü–û–õ–¨–ó–£–ï–ú –¢–ò–ü –û–ë–™–ï–ö–¢–ê –ò–ó –í–û–ü–†–û–°–ê (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω —è–≤–Ω–æ):
   - "–≠–¢–û–¢ –§–†–ê–ù–¶–£–ó –Ω–µ –æ–±–ª–∞–¥–∞–ª" ‚Üí form: "—Ñ—Ä–∞–Ω—Ü—É–∑"
   - "–≠–¢–ê –°–¢–û–õ–ò–¶–ê –Ω–∞—Ö–æ–¥–∏—Ç—Å—è" ‚Üí form: "—Å—Ç–æ–ª–∏—Ü–∞"
   - "–≠–¢–û–¢ –ì–û–†–û–î –°–®–ê —Å—Ç–∞–ª" ‚Üí form: "—ç—Ç–æ—Ç –≥–æ—Ä–æ–¥"
   - "–≠–¢–ò–ú –ê–ù–ì–õ–ò–ô–°–ö–ò–ú –í–´–†–ê–ñ–ï–ù–ò–ï–ú –Ω–∞–∑—ã–≤–∞—é—Ç" ‚Üí form: "–∞–Ω–≥–ª–∏–π—Å–∫–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ"
   - "–≠–¢–û–ú–£ –ü–†–û–ò–ó–í–ï–î–ï–ù–ò–Æ –ø–æ—Å–≤—è—â–µ–Ω–æ" ‚Üí form: "–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ"

4Ô∏è‚É£ –ï–°–õ–ò –í PDF –ï–°–¢–¨ –°–¢–†–û–ö–ê "–ó–∞—á–µ—Ç:":
   - –ò–∑–≤–ª–µ–∫–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ "accept": ["–≤–∞—Ä–∏–∞–Ω—Ç1", "–≤–∞—Ä–∏–∞–Ω—Ç2"]
   - –ù–û form –≤—Å—ë —Ä–∞–≤–Ω–æ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏–µ–º/–º–∞—Ä–∫–µ—Ä–æ–º –∏–∑ –≤–æ–ø—Ä–æ—Å–∞!

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
–†–ï–ê–õ–¨–ù–´–ï –ü–†–ò–ú–ï–†–´ –ò–ó –ü–ê–ö–ï–¢–û–í:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û:
–í–æ–ø—Ä–æ—Å: "–í –¥–µ–∫–∞–±—Ä–µ 2012 –≥–æ–¥–∞ –í –ß–ï–°–¢–¨ –ù–ï–ì–û –Ω–∞–∑–≤–∞–ª–∏ –∏—Å–∫–æ–ø–∞–µ–º—É—é —è—â–µ—Ä–∏—Ü—É"
‚Üí form: "–≤ —á–µ—Å—Ç—å –Ω–µ–≥–æ"
‚Üí answer: "–ë–∞—Ä–∞–∫ –û–±–∞–º–∞"

‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û:
–í–æ–ø—Ä–æ—Å: "–≠—Å—Ç–æ–Ω–µ—Ü —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –Ω–∞—Ä–∞–≤–Ω–µ —Å —Ñ–∞–º–∏–ª–∏–µ–π –≠–¢–û–ì–û –ì–û–õ–õ–ê–ù–î–¶–ê"
‚Üí form: "–≥–æ–ª–ª–∞–Ω–¥–µ—Ü"
‚Üí answer: "–Ø–Ω –•–µ–Ω–¥—Ä–∏–∫ –û–æ—Ä—Ç"

‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û:
–í–æ–ø—Ä–æ—Å: "–í –æ—Ç–≤–µ—Ç–µ –æ–¥–Ω–æ —Å–ª–æ–≤–æ. ¬´–¢—Ä–µ—Ç—å–µ–π –ï–ô¬ª –Ω–∞–∑—ã–≤–∞—é—Ç –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ"
‚Üí form: "–æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º, –æ–Ω–∞"
‚Üí answer: "—Ä—É–∫–∞"

‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û:
–í–æ–ø—Ä–æ—Å: "–û–ù –±—ã–ª –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–æ–º –∫–∞—Ñ–µ–¥—Ä—ã –±–∏–æ—Ö–∏–º–∏–∏"
‚Üí form: "–æ–Ω"
‚Üí answer: "–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –û–ø–∞—Ä–∏–Ω"

‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û:
–í–æ–ø—Ä–æ—Å: "–≠–¢–ê –°–¢–û–õ–ò–¶–ê –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ 25 –∫–º –æ—Ç —ç–∫–≤–∞—Ç–æ—Ä–∞"
‚Üí form: "—Å—Ç–æ–ª–∏—Ü–∞"
‚Üí answer: "–ö–∏—Ç–æ"

‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û:
–í–æ–ø—Ä–æ—Å: "–î–µ–≤—É—à–∫–∏ –æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –ò–ú–ò"
‚Üí form: "–∏–º–∏"
‚Üí answer: "–≤–∞–º–ø–∏—Ä—ã"

‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û:
‚Üí form: "–ë–∞—Ä–∞–∫ –û–±–∞–º–∞" ‚ùå (—ç—Ç–æ –æ—Ç–≤–µ—Ç, –Ω–µ —Ñ–æ—Ä–º–∞!)
‚Üí form: "–æ—Ç–≤–µ—Ç" ‚ùå (–±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω–æ)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
–§–û–†–ú–ê–¢ –í–´–•–û–î–ù–û–ì–û JSON:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

{
  "info": "–ø–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–∫–µ—Ç–µ: –Ω–∞–∑–≤–∞–Ω–∏–µ, –∞–≤—Ç–æ—Ä—ã, —Ä–µ–¥–∞–∫—Ç–æ—Ä—ã, –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏, –º–æ—Ä–∞—Ç–æ—Ä–∏–π",
  "package_name": "–Ω–∞–∑–≤–∞–Ω–∏–µ –ø–∞–∫–µ—Ç–∞",
  "themes": [
    {
      "name": "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã (–ê–≤—Ç–æ—Ä: –ò–º—è –§–∞–º–∏–ª–∏—è)",
      "theme_comment": "–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∫ —Ç–µ–º–µ, –µ—Å–ª–∏ –µ—Å—Ç—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ (–ù–ï –≤—ã–¥—É–º—ã–≤–∞—Ç—å!)",
      "questions": [
        {
          "cost": 10,
          "question": "—Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞ —Å –ó–ê–ì–õ–ê–í–ù–´–ú–ò –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è–º–∏",
          "form": "–º–µ—Å—Ç–æ–∏–º–µ–Ω–∏–µ/–º–∞—Ä–∫–µ—Ä –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ (–û–ù/–û–ù–ê/–ï–ì–û/–ò–•/–≥–æ—Ä–æ–¥/—Å—Ç–æ–ª–∏—Ü–∞ –∏ —Ç.–¥.)",
          "answer": "—Å–∞–º –æ—Ç–≤–µ—Ç",
          "source": "–∏—Å—Ç–æ—á–Ω–∏–∫ –æ—Ç–≤–µ—Ç–∞ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)",
          "comment": "–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∫ –æ—Ç–≤–µ—Ç—É",
          "accept": ["–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞1", "–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞2"]  // –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –µ—Å—Ç—å "–ó–∞—á–µ—Ç:"
        }
      ]
    }
  ]
}

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
–û–ë–©–ò–ï –ü–†–ê–í–ò–õ–ê –ü–ê–†–°–ò–ù–ì–ê:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

1. –°—Ç—Ä—É–∫—Ç—É—Ä–∞: –ë–æ–∏/–†–∞—É–Ω–¥—ã ‚Üí –¢–µ–º—ã ‚Üí –í–æ–ø—Ä–æ—Å—ã (10, 20, 30, 40, 50 –æ—á–∫–æ–≤)
2. Info –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∏–∑ –Ω–∞—á–∞–ª–∞ —Ñ–∞–π–ª–∞ (–∞–≤—Ç–æ—Ä—ã, —Ä–µ–¥–∞–∫—Ç–æ—Ä—ã, –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏, –º–æ—Ä–∞—Ç–æ—Ä–∏–π)
3. –°–æ—Ö—Ä–∞–Ω—è–π –≤—Å–µ —É–¥–∞—Ä–µ–Ω–∏—è, —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
4. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π –±–µ–ª–æ—Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ ("–ê–¥–∫–∞–∑:", "–ö–∞–º–µ–Ω—Ç–∞—Ä:")
5. –ï—Å–ª–∏ –≤ –≤–æ–ø—Ä–æ—Å–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏–π, –≤—ã–±–∏—Ä–∞–π –æ—Å–Ω–æ–≤–Ω–æ–µ (–æ–±—ã—á–Ω–æ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø–µ—Ä–µ–¥ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–æ–º)
6. –í–°–ï–ì–î–ê –∏—â–∏ –∏ –¥–æ–±–∞–≤–ª—è–π "source" (–∏—Å—Ç–æ—á–Ω–∏–∫ –æ—Ç–≤–µ—Ç–∞) –µ—Å–ª–∏ –æ–Ω —É–∫–∞–∑–∞–Ω –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ:
   - –ò—Å—Ç–æ—á–Ω–∏–∫ –æ–±—ã—á–Ω–æ –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ –≤ —Å–∫–æ–±–∫–∞—Ö: "(–∏—Å—Ç–æ—á–Ω–∏–∫: ...)", "–ò—Å—Ç–æ—á–Ω–∏–∫: ...", "[...]"
   - –ï—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –Ω–µ—Ç, –º–æ–∂–Ω–æ –æ–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–µ
7. –ö–†–ò–¢–ò–ß–ù–û: –í JSON —Å—Ç—Ä–æ–∫–∞—Ö –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —ç–∫—Ä–∞–Ω–∏—Ä—É–π:
   - –ö–∞–≤—ã—á–∫–∏: \\"
   - –ü–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫: \\n
   - –û–±—Ä–∞—Ç–Ω—ã–π —Å–ª–µ—à: \\\\

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON, –±–µ–∑ markdown –±–ª–æ–∫–æ–≤ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""

def format_duration(seconds: float) -> str:
    """Format duration in human-readable format"""
    if seconds < 60:
        return f"{seconds:.1f}s"
    minutes = int(seconds // 60)
    secs = seconds % 60
    return f"{minutes}m {secs:.1f}s"


def convert_docx_to_pdf(docx_path: str) -> str:
    """Convert DOCX to PDF using LibreOffice or python-docx + reportlab"""
    import tempfile
    import subprocess
    
    docx_path_obj = Path(docx_path)
    pdf_path = docx_path_obj.with_suffix('.pdf')

    try:
        subprocess.run(
            ['libreoffice', '--headless', '--convert-to', 'pdf', '--outdir', str(docx_path_obj.parent), str(docx_path)],
            check=True,
            capture_output=True,
            timeout=30
        )
        if pdf_path.exists():
            return str(pdf_path)
    except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
        pass

    try:
        from docx import Document
        from reportlab.lib.pagesizes import A4
        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.lib.units import inch
        from reportlab.pdfbase import pdfmetrics
        from reportlab.pdfbase.ttfonts import TTFont
        from reportlab.lib.enums import TA_LEFT
        import re

        print(f"  Converting DOCX to PDF using python-docx + reportlab...")

        doc = Document(docx_path)

        font_registered = False
        font_name = None

        fonts_to_try = [
            ('/System/Library/Fonts/Supplemental/Arial Unicode.ttf', 'ArialUnicode'),
            ('/System/Library/Fonts/Helvetica.ttc', 'Helvetica'),
            ('/Library/Fonts/Arial.ttf', 'Arial'),
            ('/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf', 'DejaVuSans'),
            ('/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', 'LiberationSans'),
            ('C:/Windows/Fonts/arial.ttf', 'Arial'),
            ('C:/Windows/Fonts/calibri.ttf', 'Calibri'),
        ]

        for font_path, fname in fonts_to_try:
            if os.path.exists(font_path):
                try:
                    pdfmetrics.registerFont(TTFont(fname, font_path))
                    font_name = fname
                    font_registered = True
                    print(f"  ‚úì Registered font: {fname}")
                    break
                except Exception as e:
                    continue

        if not font_registered:
            print("  ‚ö† Warning: Could not register Unicode font, falling back to default (may not display Cyrillic properly)")
            font_name = 'Helvetica'

        pdf_doc = SimpleDocTemplate(str(pdf_path), pagesize=A4,
                                    rightMargin=72, leftMargin=72,
                                    topMargin=72, bottomMargin=18)

        elements = []
        styles = getSampleStyleSheet()

        normal_style = ParagraphStyle(
            'CustomNormal',
            parent=styles['Normal'],
            fontName=font_name,
            fontSize=10,  # Reduced from 11
            leading=12,   # Reduced from 14
            spaceAfter=3, # Reduced from 6
            alignment=TA_LEFT,
        )

        heading_style = ParagraphStyle(
            'CustomHeading',
            parent=styles['Heading2'],
            fontName=font_name,
            fontSize=12,  # Reduced from 14
            leading=14,   # Reduced from 16
            spaceAfter=6, # Reduced from 12
            alignment=TA_LEFT,
        )

        for para in doc.paragraphs:
            text = para.text.strip()
            if not text:
                continue

            text = text.replace('\n', '<br/>')
            text = text.replace('&', '&amp;')
            text = text.replace('<', '&lt;')
            text = text.replace('>', '&gt;')

            is_heading = False
            if para.runs:
                first_run = para.runs[0]
                if first_run.bold or (first_run.font.size and first_run.font.size.pt > 12):
                    is_heading = True

            style = heading_style if is_heading else normal_style

            try:
                p = Paragraph(text, style)
                elements.append(p)
                if is_heading:
                    elements.append(Spacer(1, 3))
            except Exception as e:
                try:
                    p = Paragraph(text.replace('<br/>', ' '), normal_style)
                    elements.append(p)
                except:
                    print(f"  ‚ö† Warning: Could not add paragraph: {text[:50]}...")
                    pass

        pdf_doc.build(elements)
        
        if Path(pdf_path).exists() and Path(pdf_path).stat().st_size > 0:
            print(f"  ‚úì PDF created: {pdf_path.name}")
            return str(pdf_path)
        else:
            raise ValueError("PDF file was not created or is empty")
            
    except ImportError as e:
        print(f"  Missing dependencies: {e}")
        print("  Install: pip install python-docx reportlab")
        raise
    except Exception as e:
        print(f"  Error during conversion: {e}")
        raise
    
    raise ValueError(
        f"Cannot convert DOCX to PDF. Install dependencies:\n"
        f"  pip install python-docx reportlab\n"
        f"\nOr install LibreOffice (better quality):\n"
        f"  brew install --cask libreoffice"
    )


def merge_docx(docx_files: list[Path], output_path: str) -> None:
    """Merge multiple DOCX files into one"""
    from docx import Document

    if not docx_files:
        raise ValueError("No DOCX files to merge")

    merged_doc = Document()

    for i, docx_path in enumerate(docx_files):
        doc = Document(str(docx_path))

        if i > 0:
            merged_doc.add_page_break()

        from docx.oxml import parse_xml
        from docx.oxml.ns import qn

        for para in doc.paragraphs:
            para_xml_str = para._element.xml
            new_para_xml = parse_xml(para_xml_str)

            hyperlinks = new_para_xml.xpath('.//w:hyperlink')
            for hyperlink_elem in hyperlinks:
                rel_id = hyperlink_elem.get(qn('r:id'))
                if rel_id and rel_id in doc.part.rels:
                    source_rel = doc.part.rels[rel_id]
                    existing_rel_id = None
                    for r_id, r in merged_doc.part.rels.items():
                        if r.target_ref == source_rel.target_ref:
                            existing_rel_id = r_id
                            break

                    if existing_rel_id:
                        hyperlink_elem.set(qn('r:id'), existing_rel_id)
                    else:
                        new_rel = merged_doc.part.rels.get_or_add_ext_rel(
                            source_rel.reltype,
                            source_rel.target_ref
                        )
                        hyperlink_elem.set(qn('r:id'), new_rel.rId)

            merged_doc.element.body.append(new_para_xml)

        for table in doc.tables:
            new_table = merged_doc.add_table(rows=len(table.rows), cols=len(table.columns))
            for i, row in enumerate(table.rows):
                for j, cell in enumerate(row.cells):
                    new_table.rows[i].cells[j].text = cell.text
    
    merged_doc.save(output_path)


def merge_pdfs(pdf_files: list[Path], output_path: str) -> None:
    """Merge multiple PDF files into one"""
    try:
        from pypdf import PdfReader, PdfWriter
        
        writer = PdfWriter()
        for pdf_path in pdf_files:
            reader = PdfReader(str(pdf_path))
            for page in reader.pages:
                writer.add_page(page)
        with open(output_path, 'wb') as output_file:
            writer.write(output_file)
    except ImportError:
        try:
            from PyPDF2 import PdfMerger
            merger = PdfMerger()
            for pdf_path in pdf_files:
                merger.append(str(pdf_path))
            merger.write(output_path)
            merger.close()
        except ImportError:
            raise ImportError("Need pypdf or PyPDF2: pip install pypdf")


def merge_folder_to_docx(folder_path: str) -> str:
    """Merge all DOCX files from folder into one"""
    folder = Path(folder_path)

    docx_files = sorted([f for f in folder.glob('*.docx') if not f.name.startswith('~$')])

    if not docx_files:
        raise ValueError(f"No DOCX files found in {folder_path}")

    print(f"Found {len(docx_files)} DOCX files")

    merged_docx_name = f"{folder.name}_merged.docx"
    merged_docx_path = folder / merged_docx_name
    
    print(f"Merging {len(docx_files)} DOCX files into one...")
    merge_docx(docx_files, str(merged_docx_path))
    
    print(f"‚úì Merged DOCX created: {merged_docx_path}")
    return str(merged_docx_path)


def extract_text_from_pdf(pdf_path: str) -> str:
    """Extract text from PDF file"""
    try:
        from pypdf import PdfReader

        reader = PdfReader(pdf_path)
        text = []
        for page in reader.pages:
            text.append(page.extract_text())
        return "\n\n".join(text)
    except ImportError:
        try:
            from PyPDF2 import PdfReader

            reader = PdfReader(pdf_path)
            text = []
            for page in reader.pages:
                text.append(page.extract_text())
            return "\n\n".join(text)
        except ImportError:
            raise ImportError("Need pypdf or PyPDF2: pip install pypdf")


def extract_text_from_docx(docx_path: str) -> str:
    """Extract text from DOCX file"""
    try:
        from docx import Document

        doc = Document(docx_path)
        text = []
        for para in doc.paragraphs:
            if para.text.strip():
                text.append(para.text)
        return "\n".join(text)
    except ImportError:
        raise ImportError("Need python-docx: pip install python-docx")


def extract_text(file_path: str) -> str:
    """Extract text from PDF or DOCX file"""
    file_path_obj = Path(file_path)

    if file_path_obj.suffix.lower() == '.pdf':
        return extract_text_from_pdf(file_path)
    elif file_path_obj.suffix.lower() == '.docx':
        return extract_text_from_docx(file_path)
    else:
        raise ValueError(f"Unsupported file type: {file_path_obj.suffix}")


def upload_file_to_gemini(file_path: str):
    """Upload file (PDF or DOCX) to Gemini"""
    file_path_obj = Path(file_path)

    if file_path_obj.suffix.lower() == '.pdf':
        mime_type = 'application/pdf'
        display_name = 'quiz_pack.pdf'
        file_type = 'PDF'
    elif file_path_obj.suffix.lower() == '.docx':
        mime_type = 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        display_name = 'quiz_pack.docx'
        file_type = 'DOCX'
    else:
        raise ValueError(f"Unsupported file type: {file_path_obj.suffix}. Supported: PDF, DOCX")

    print(f"Uploading {file_type}: {file_path_obj.name}")

    with open(file_path, 'rb') as f:
        file = client.files.upload(
            file=f,
            config=types.UploadFileConfig(
                mime_type=mime_type,
                display_name=display_name
            )
        )

    print(f"File uploaded: {file.name}")

    while file.state == types.FileState.PROCESSING:
        print("Processing file...")
        time.sleep(2)
        file = client.files.get(name=file.name)

    if file.state == types.FileState.FAILED:
        raise ValueError("File processing failed")

    print("File ready for analysis")
    return file

def extract_json_from_response(response_text: str):
    """Extract and parse JSON from Gemini response (returns dict or list)"""
    if response_text is None:
        raise ValueError("Response text is None - API may have failed or text was too large")

    response_text = response_text.strip()

    if response_text.startswith("```json"):
        response_text = response_text[7:]
    elif response_text.startswith("```"):
        response_text = response_text[3:]

    if response_text.endswith("```"):
        response_text = response_text[:-3]

    response_text = response_text.strip()

    try:
        return json.loads(response_text)
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON: {e}")
        print(f"Response text (first 1000 chars): {response_text[:1000]}")
        print(f"Response text (last 500 chars): {response_text[-500:]}")

        print("Attempting to fix JSON...")

        try:
            return json.loads(response_text, strict=False)
        except:
            pass

        debug_path = Path("debug_response.txt")
        debug_path.write_text(response_text, encoding='utf-8')
        print(f"Full response saved to: {debug_path}")

        raise ValueError(f"Failed to parse JSON. Saved full response to {debug_path}") from e

def get_package_structure(uploaded_file=None, text_content: str = None, is_merged: bool = False) -> dict:
    """Get package metadata and theme names (first pass)"""
    print("Step 1/2: Getting package structure...")

    merged_note = """
–û–°–û–ë–ï–ù–ù–û–°–¢–ò –û–ë–™–ï–î–ò–ù–Å–ù–ù–û–ì–û –§–ê–ô–õ–ê (–Ω–µ—Å–∫–æ–ª—å–∫–æ "–±–æ—ë–≤"):
- –§–∞–π–ª –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ "–±–æ—ë–≤" –∏–ª–∏ "—Ä–∞—É–Ω–¥–æ–≤" (—Ä–∞–∑–¥–µ–ª–µ–Ω—ã –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ —Ç–∏–ø–∞ "–ë–æ–π 1", "–ë–æ–π 2")
- –ü–∞—Ä—Å–∏ –í–°–ï —Ç–µ–º—ã –∏–∑ –í–°–ï–• –±–æ—ë–≤, –Ω–µ –ø—Ä–æ–ø—É—Å–∫–∞–π –Ω–∏—á–µ–≥–æ
- –ò–≥–Ω–æ—Ä–∏—Ä—É–π –∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª—ã, –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü, —Å–ª—É–∂–µ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –º–µ–∂–¥—É –±–æ—è–º–∏
""" if is_merged else ""

    prompt = f"""–ò–∑–≤–ª–µ–∫–∏ –∏–∑ {"—Ç–µ–∫—Å—Ç–∞" if text_content else "—Ñ–∞–π–ª–∞ (PDF –∏–ª–∏ DOCX)"} –¢–û–õ–¨–ö–û –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ —Å–ø–∏—Å–æ–∫ —Ç–µ–º (–ë–ï–ó –≤–æ–ø—Ä–æ—Å–æ–≤).

{merged_note}

–í–µ—Ä–Ω–∏ JSON:
{{
  "info": "–ø–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–∫–µ—Ç–µ: –Ω–∞–∑–≤–∞–Ω–∏–µ, –∞–≤—Ç–æ—Ä—ã, —Ä–µ–¥–∞–∫—Ç–æ—Ä—ã, –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏, –º–æ—Ä–∞—Ç–æ—Ä–∏–π",
  "package_name": "–Ω–∞–∑–≤–∞–Ω–∏–µ –ø–∞–∫–µ—Ç–∞",
  "themes": [
    {{"name": "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã 1 (–ê–≤—Ç–æ—Ä: –ò–º—è)"}},
    {{"name": "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã 2 (–ê–≤—Ç–æ—Ä: –ò–º—è)"}}
  ]
}}

–í–ê–ñ–ù–û:
- –ù–ï –≤–∫–ª—é—á–∞–π –≤–æ–ø—Ä–æ—Å—ã, —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–µ–º
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–∫–µ—Ç–µ (info) –ù–ï –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ, –æ—Å—Ç–∞–≤—å –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π: ""
- –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–∞–∫–µ—Ç–∞ (package_name) –ù–ï –Ω–∞–π–¥–µ–Ω–æ, –ø–æ–ø—Ä–æ–±—É–π –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ (–Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞, –∑–∞–≥–æ–ª–æ–≤–∫–∏) –∏–ª–∏ –æ—Å—Ç–∞–≤—å –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π: ""
- –ù–ï –≤—ã–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
- –í JSON —Å—Ç—Ä–æ–∫–∞—Ö –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —ç–∫—Ä–∞–Ω–∏—Ä—É–π –∫–∞–≤—ã—á–∫–∏ –∫–∞–∫ \\" –∏ –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫ –∫–∞–∫ \\n
- –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –±–µ–∑ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫"""

    # Build contents based on mode (file or text)
    if text_content:
        contents = [f"{prompt}\n\n--- –¢–ï–ö–°–¢ –î–û–ö–£–ú–ï–ù–¢–ê ---\n{text_content}"]
    else:
        contents = [
            types.Part.from_uri(file_uri=uploaded_file.uri, mime_type=uploaded_file.mime_type),
            prompt
        ]

    response = client.models.generate_content(
        model="gemini-2.5-pro",
        contents=contents,
        config=types.GenerateContentConfig(
            temperature=0.1,
            max_output_tokens=8192
        )
    )

    if not response.text:
        print("\n‚ö†Ô∏è  ERROR: Response text is None!")
        print(f"Response object: {response}")

        if hasattr(response, 'candidates') and response.candidates:
            print(f"Number of candidates: {len(response.candidates)}")
            for i, candidate in enumerate(response.candidates):
                print(f"  Candidate {i}:")
                if hasattr(candidate, 'finish_reason'):
                    print(f"    Finish reason: {candidate.finish_reason}")
                if hasattr(candidate, 'safety_ratings'):
                    print(f"    Safety ratings: {candidate.safety_ratings}")
                if hasattr(candidate, 'content'):
                    print(f"    Has content: {candidate.content is not None}")

        if hasattr(response, 'prompt_feedback'):
            print(f"Prompt feedback: {response.prompt_feedback}")

        raise ValueError(
            "Response text is None. Possible causes:\n"
            "1. Content was blocked by safety filters\n"
            "2. Output exceeded max_output_tokens limit\n"
            "3. API error occurred\n"
            "Check the diagnostics above for details."
        )

    return extract_json_from_response(response.text)

def parse_theme_questions(uploaded_file=None, text_content: str = None, theme_name: str = None, theme_index: int = 0, is_merged: bool = False) -> list:
    """Parse questions for a specific theme (second pass)"""
    print(f"  Parsing theme {theme_index + 1}: {theme_name[:50]}...")

    merged_note = """
–û–°–û–ë–ï–ù–ù–û–°–¢–ò –û–ë–™–ï–î–ò–ù–Å–ù–ù–û–ì–û –§–ê–ô–õ–ê:
- –ï—Å–ª–∏ –Ω–æ–º–µ—Ä–∞ –≤–æ–ø—Ä–æ—Å–æ–≤ (10, 20, 30) –Ω–µ —è–≤–Ω—ã–µ, –æ–ø—Ä–µ–¥–µ–ª—è–π cost –ø–æ –ø–æ—Ä—è–¥–∫—É:
  * –ü–µ—Ä–≤—ã–π –≤–æ–ø—Ä–æ—Å –≤ —Ç–µ–º–µ = cost: 10
  * –í—Ç–æ—Ä–æ–π –≤–æ–ø—Ä–æ—Å = cost: 20
  * –ò —Ç.–¥.
""" if is_merged else ""

    prompt = f"""–ò–∑–≤–ª–µ–∫–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ–º—ã "{theme_name}" (—Ç–µ–º–∞ #{theme_index + 1} –≤ –ø–∞–∫–µ—Ç–µ).

–§–û–†–ú–ê–¢ –ü–û–õ–Ø "form" (–º–∞—Ä–∫–µ—Ä –∏–∑ –≤–æ–ø—Ä–æ—Å–∞, –ù–ï –æ—Ç–≤–µ—Ç!):
- "–û–ù –±—ã–ª –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–æ–º" ‚Üí form: "–æ–Ω"
- "–í –ß–ï–°–¢–¨ –ù–ï–ì–û –Ω–∞–∑–≤–∞–ª–∏" ‚Üí form: "–≤ —á–µ—Å—Ç—å –Ω–µ–≥–æ"
- "–≠–¢–û–¢ –§–†–ê–ù–¶–£–ó" ‚Üí form: "—Ñ—Ä–∞–Ω—Ü—É–∑"
- "–í –æ—Ç–≤–µ—Ç–µ –æ–¥–Ω–æ —Å–ª–æ–≤–æ. –û–ù–ê" ‚Üí form: "–æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º, –æ–Ω–∞"
- –ï—Å–ª–∏ –≤ –≤–æ–ø—Ä–æ—Å–µ –ù–ï–¢ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏–π, –∏–∑–≤–ª–µ–∫–∞–π form –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:
  * "–ß—Ç–æ –º—ã –∑–∞–º–µ–Ω–∏–ª–∏" ‚Üí form: "—á—Ç–æ –º—ã –∑–∞–º–µ–Ω–∏–ª–∏"
  * "–°–∫–∞–∂—ã—Ü–µ –ø–∞-—û–∫—Ä–∞—ñ–Ω—Å–∫—ñ" ‚Üí form: "–ø–∞-—û–∫—Ä–∞—ñ–Ω—Å–∫—ñ"
  * "–ù–∞–∑–æ–≤–∏—Ç–µ" ‚Üí form: "–Ω–∞–∑–æ–≤–∏—Ç–µ"
  * "–£–∫–∞–∂–∏—Ç–µ" ‚Üí form: "—É–∫–∞–∂–∏—Ç–µ"
  * "–í –æ—Ç–≤–µ—Ç–µ –æ–¥–Ω–æ —Å–ª–æ–≤–æ" ‚Üí form: "–æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º"

{merged_note}

–í–µ—Ä–Ω–∏ JSON –æ–±—ä–µ–∫—Ç:
{{
  "theme_comment": "–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∫ —Ç–µ–º–µ, –µ—Å–ª–∏ –µ—Å—Ç—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ (–ù–ï –≤—ã–¥—É–º—ã–≤–∞–π!)",
  "questions": [
    {{"cost": 10, "question": "...", "form": "–æ–Ω/–æ–Ω–∞/–µ–≥–æ/–≥–æ—Ä–æ–¥", "answer": "...", "source": "–∏—Å—Ç–æ—á–Ω–∏–∫"}},
    {{"cost": 20, "question": "...", "form": "...", "answer": "...", "source": "..."}},
    ...
  ]
}}

–í–ê–ñ–ù–û:
- theme_comment: –¥–æ–±–∞–≤–ª—è–π –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –µ—Å—Ç—å –ø–æ—è—Å–Ω–µ–Ω–∏–µ –∫ —Ç–µ–º–µ (–ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º). –ù–ï –≤—ã–¥—É–º—ã–≤–∞–π!
- –í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ cost (10, 20, 30, 40, 50)
- –í–°–ï–ì–î–ê –∏—â–∏ –∏ –¥–æ–±–∞–≤–ª—è–π –ø–æ–ª–µ "source" (–∏—Å—Ç–æ—á–Ω–∏–∫ –æ—Ç–≤–µ—Ç–∞) –µ—Å–ª–∏ –æ–Ω–æ —É–∫–∞–∑–∞–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
- –ò—Å—Ç–æ—á–Ω–∏–∫ –æ–±—ã—á–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ –≤ —Å–∫–æ–±–∫–∞—Ö –∏–ª–∏ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π (–ø—Ä–∏–º–µ—Ä—ã: "(–∏—Å—Ç–æ—á–Ω–∏–∫: ...)", "–ò—Å—Ç–æ—á–Ω–∏–∫: ...", "[...]")
- –í–∫–ª—é—á–∞–π "comment" –∏ "accept" —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –≤ PDF
- –í JSON —Å—Ç—Ä–æ–∫–∞—Ö –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —ç–∫—Ä–∞–Ω–∏—Ä—É–π –∫–∞–≤—ã—á–∫–∏ –∫–∞–∫ \\" –∏ –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫ –∫–∞–∫ \\n
- –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç –±–µ–∑ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫"""

    # Build contents based on mode (file or text)
    if text_content:
        contents = [f"{prompt}\n\n--- –¢–ï–ö–°–¢ –î–û–ö–£–ú–ï–ù–¢–ê ---\n{text_content}"]
    else:
        contents = [
            types.Part.from_uri(file_uri=uploaded_file.uri, mime_type=uploaded_file.mime_type),
            prompt
        ]

    response = client.models.generate_content(
        model="gemini-2.5-pro",
        contents=contents,
        config=types.GenerateContentConfig(
            temperature=0.1,
            max_output_tokens=8192
        )
    )

    if not response.text:
        print(f"\n‚ö†Ô∏è  ERROR: Response text is None for theme: {theme_name}")
        print(f"Response object: {response}")

        if hasattr(response, 'candidates') and response.candidates:
            print(f"Number of candidates: {len(response.candidates)}")
            for i, candidate in enumerate(response.candidates):
                print(f"  Candidate {i}:")
                if hasattr(candidate, 'finish_reason'):
                    print(f"    Finish reason: {candidate.finish_reason}")
                if hasattr(candidate, 'safety_ratings'):
                    print(f"    Safety ratings: {candidate.safety_ratings}")
                if hasattr(candidate, 'content'):
                    print(f"    Has content: {candidate.content is not None}")

        if hasattr(response, 'prompt_feedback'):
            print(f"Prompt feedback: {response.prompt_feedback}")

        raise ValueError(
            f"Response text is None for theme '{theme_name}'. Possible causes:\n"
            "1. Content was blocked by safety filters\n"
            "2. Output exceeded max_output_tokens limit\n"
            "3. API error occurred\n"
            "Check the diagnostics above for details."
        )

    result = extract_json_from_response(response.text)

    # Handle both old format (list) and new format (dict with theme_comment and questions)
    if isinstance(result, list):
        return {"questions": result}
    elif isinstance(result, dict) and "questions" in result:
        return result
    else:
        raise ValueError(f"Expected dict with questions or list, got {type(result)}")

def parse_with_text_chunked(file_path: str, is_merged: bool = False) -> dict:
    """Parse file using plain text extraction and Gemini API (faster, no file upload)"""

    total_start = time.time()

    extract_start = time.time()
    print(f"Extracting text from: {Path(file_path).name}")
    text_content = extract_text(file_path)
    extract_duration = time.time() - extract_start
    print(f"  ‚è± Text extraction took: {format_duration(extract_duration)}")
    print(f"  Text length: {len(text_content)} chars")

    structure_start = time.time()
    structure = get_package_structure(text_content=text_content, is_merged=is_merged)
    structure_duration = time.time() - structure_start
    print(f"  ‚è± Structure extraction took: {format_duration(structure_duration)}")

    if "themes" not in structure or not structure["themes"]:
        raise ValueError("Failed to extract themes from text")

    print(f"Found {len(structure['themes'])} themes")
    print(f"Step 2/2: Parsing questions for each theme...")

    themes_start = time.time()
    theme_times = []
    failed_themes = []

    for i, theme in enumerate(structure["themes"]):
        theme_start = time.time()
        theme_name = theme.get("name", f"Theme {i+1}")
        try:
            result = parse_theme_questions(text_content=text_content, theme_name=theme_name, theme_index=i, is_merged=is_merged)
            theme["questions"] = result.get("questions", [])
            if result.get("theme_comment"):
                theme["theme_comment"] = result["theme_comment"]
            theme_duration = time.time() - theme_start
            theme_times.append(theme_duration)
            print(f"    ‚úì {len(theme['questions'])} questions parsed ({format_duration(theme_duration)})")
        except Exception as e:
            theme["questions"] = []
            failed_themes.append((i + 1, theme_name, str(e)))
            print(f"    ‚ö†Ô∏è FAILED: {theme_name[:50]}... - {e}")
            continue

    themes_duration = time.time() - themes_start
    total_duration = time.time() - total_start

    print()
    print("=" * 50)
    print("‚è± TIMING SUMMARY (TEXT MODE):")
    print(f"  Text extraction:     {format_duration(extract_duration)}")
    print(f"  Structure extraction: {format_duration(structure_duration)}")
    print(f"  Themes parsing:      {format_duration(themes_duration)}")
    if theme_times:
        avg_theme = sum(theme_times) / len(theme_times)
        print(f"    Avg per theme:     {format_duration(avg_theme)}")
    print(f"  TOTAL:               {format_duration(total_duration)}")
    if failed_themes:
        print(f"  ‚ö†Ô∏è FAILED THEMES:    {len(failed_themes)}")
        for num, name, err in failed_themes:
            print(f"    - Theme {num}: {name[:40]}...")
    print("=" * 50)
    print()

    return structure


def parse_pdf_with_gemini_chunked(file_path: str, is_merged: bool = False) -> dict:
    """Parse PDF/DOCX file using Gemini API with chunked approach (recommended for large files)"""

    total_start = time.time()

    upload_start = time.time()
    uploaded_file = upload_file_to_gemini(file_path)
    upload_duration = time.time() - upload_start
    print(f"  ‚è± Upload took: {format_duration(upload_duration)}")

    structure_start = time.time()
    structure = get_package_structure(uploaded_file, is_merged=is_merged)
    structure_duration = time.time() - structure_start
    print(f"  ‚è± Structure extraction took: {format_duration(structure_duration)}")

    if "themes" not in structure or not structure["themes"]:
        raise ValueError("Failed to extract themes from PDF")

    print(f"Found {len(structure['themes'])} themes")
    print(f"Step 2/2: Parsing questions for each theme...")

    themes_start = time.time()
    theme_times = []
    failed_themes = []

    for i, theme in enumerate(structure["themes"]):
        theme_start = time.time()
        theme_name = theme.get("name", f"Theme {i+1}")
        try:
            result = parse_theme_questions(uploaded_file, theme_name=theme_name, theme_index=i, is_merged=is_merged)
            theme["questions"] = result.get("questions", [])
            if result.get("theme_comment"):
                theme["theme_comment"] = result["theme_comment"]
            theme_duration = time.time() - theme_start
            theme_times.append(theme_duration)
            print(f"    ‚úì {len(theme['questions'])} questions parsed ({format_duration(theme_duration)})")
        except Exception as e:
            theme["questions"] = []
            failed_themes.append((i + 1, theme_name, str(e)))
            print(f"    ‚ö†Ô∏è FAILED: {theme_name[:50]}... - {e}")
            continue

    themes_duration = time.time() - themes_start
    total_duration = time.time() - total_start

    print()
    print("=" * 50)
    print("‚è± TIMING SUMMARY (FILE MODE):")
    print(f"  Upload:              {format_duration(upload_duration)}")
    print(f"  Structure extraction: {format_duration(structure_duration)}")
    print(f"  Themes parsing:      {format_duration(themes_duration)}")
    if theme_times:
        avg_theme = sum(theme_times) / len(theme_times)
        print(f"    Avg per theme:     {format_duration(avg_theme)}")
    print(f"  TOTAL:               {format_duration(total_duration)}")
    if failed_themes:
        print(f"  ‚ö†Ô∏è FAILED THEMES:    {len(failed_themes)}")
        for num, name, err in failed_themes:
            print(f"    - Theme {num}: {name[:40]}...")
    print("=" * 50)
    print()

    return structure

def parse_pdf_with_gemini(file_path: str, chunked: bool = True, text_mode: bool = False, is_merged: bool = False) -> dict:
    """Parse PDF/DOCX file using Gemini API

    Args:
        file_path: Path to PDF or DOCX file
        chunked: If True, use chunked parsing (recommended for large files).
                 If False, use single-shot parsing (faster but may hit token limits).
        text_mode: If True, extract text and send as plain text (faster, no file upload).
                   If False, upload file to Gemini (supports images/formatting).
        is_merged: If True, file contains multiple "–±–æ—ë–≤" merged together
    """

    if text_mode:
        return parse_with_text_chunked(file_path, is_merged=is_merged)

    if chunked:
        return parse_pdf_with_gemini_chunked(file_path, is_merged=is_merged)

    uploaded_file = upload_file_to_gemini(file_path)

    print("Parsing file with Gemini (single-shot mode)...")

    prompt = """–†–∞—Å–ø–∞—Ä—Å–∏ —Ñ–∞–π–ª (PDF –∏–ª–∏ DOCX) —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ "–°–≤–æ—è –∏–≥—Ä–∞" –≤ JSON.

–§–û–†–ú–ê–¢ –ü–û–õ–Ø "form":
"form" - —ç—Ç–æ –º–∞—Ä–∫–µ—Ä –ß–¢–û –Ω–∞–∑–≤–∞—Ç—å (–ò–ó –í–û–ü–†–û–°–ê, –ù–ï –ò–ó –û–¢–í–ï–¢–ê!):
- "–û–ù –±—ã–ª –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–æ–º" ‚Üí form: "–æ–Ω"
- "–í –ß–ï–°–¢–¨ –ù–ï–ì–û –Ω–∞–∑–≤–∞–ª–∏" ‚Üí form: "–≤ —á–µ—Å—Ç—å –Ω–µ–≥–æ"
- "–≠–¢–û–¢ –§–†–ê–ù–¶–£–ó" ‚Üí form: "—Ñ—Ä–∞–Ω—Ü—É–∑"
- "–í –æ—Ç–≤–µ—Ç–µ –æ–¥–Ω–æ —Å–ª–æ–≤–æ. –û–ù–ê" ‚Üí form: "–æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º, –æ–Ω–∞"

JSON —Ñ–æ—Ä–º–∞—Ç:
{"info":"...", "package_name":"...", "themes":[{"name":"...","questions":[{"cost":10,"question":"...","form":"–æ–Ω/–æ–Ω–∞/–µ–≥–æ/–≥–æ—Ä–æ–¥","answer":"...","source":"–∏—Å—Ç–æ—á–Ω–∏–∫"}]}]}

–í–ê–ñ–ù–û:
- –í–°–ï–ì–î–ê –∏—â–∏ –∏ –¥–æ–±–∞–≤–ª—è–π –ø–æ–ª–µ "source" (–∏—Å—Ç–æ—á–Ω–∏–∫ –æ—Ç–≤–µ—Ç–∞) –µ—Å–ª–∏ –æ–Ω–æ —É–∫–∞–∑–∞–Ω–æ
- –û–ø—É—Å–∫–∞–π "comment" –∏ "accept" –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–∫–µ—Ç–µ (info) –ù–ï –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ, –æ—Å—Ç–∞–≤—å –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π: ""
- –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–∞–∫–µ—Ç–∞ (package_name) –ù–ï –Ω–∞–π–¥–µ–Ω–æ, –ø–æ–ø—Ä–æ–±—É–π –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∏–ª–∏ –æ—Å—Ç–∞–≤—å –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π: ""
- –ù–ï –≤—ã–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
- –í JSON –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —ç–∫—Ä–∞–Ω–∏—Ä—É–π –∫–∞–≤—ã—á–∫–∏ (\\") –∏ –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫ (\\n)
- –ë—É–¥—å –∫—Ä–∞—Ç–æ–∫. –ó–∞–∫—Ä–æ–π –≤—Å–µ —Å–∫–æ–±–∫–∏!"""

    response = client.models.generate_content(
        model="gemini-2.5-pro",
        contents=[
            types.Part.from_uri(file_uri=uploaded_file.uri, mime_type=uploaded_file.mime_type),
            prompt
        ],
        config=types.GenerateContentConfig(
            temperature=0.1,
            max_output_tokens=8192
        )
    )

    return extract_json_from_response(response.text)

def save_json(data: dict, output_path: str):
    """Save parsed data to JSON file"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"JSON saved to: {output_path}")

def validate_json_structure(data: dict) -> bool:
    """Validate that JSON has required structure and all questions have 'form' field"""
    if "info" not in data:
        print("Warning: Missing 'info' field")
        return False

    if "themes" not in data:
        print("Error: Missing 'themes' field")
        return False

    missing_forms = []
    empty_themes = []
    for theme_idx, theme in enumerate(data["themes"]):
        if "questions" not in theme:
            print(f"Error: Theme {theme_idx} missing 'questions' field")
            return False

        if len(theme["questions"]) == 0:
            empty_themes.append(f"Theme {theme_idx + 1}: {theme.get('name', 'Unknown')[:40]}...")
            continue

        for q_idx, question in enumerate(theme["questions"]):
            if "form" not in question:
                missing_forms.append(f"Theme {theme_idx}, Question {q_idx} (cost: {question.get('cost', '?')})")

    if empty_themes:
        print(f"‚ö†Ô∏è WARNING: {len(empty_themes)} theme(s) have no questions (failed to parse):")
        for item in empty_themes:
            print(f"  - {item}")

    if missing_forms:
        print("ERROR: Missing 'form' field in the following questions:")
        for item in missing_forms:
            print(f"  - {item}")
        return False

    total_questions = sum(len(t['questions']) for t in data['themes'])
    print("‚úì JSON structure validation passed")
    print(f"‚úì All {total_questions} questions have 'form' field")
    if empty_themes:
        print(f"‚ö†Ô∏è Note: {len(empty_themes)} theme(s) were skipped due to parsing errors")
    return True

def generate_short_name(text: str) -> str:
    """Generate slug from folder/package name

    Examples: "–î—Ä–æ–≤—É—à–∫–∏_2022" -> "–¥—Ä–æ–≤—É—à–∫–∏2022"
    """
    import re
    import unicodedata

    text = unicodedata.normalize('NFKD', text)

    if '.' in text:
        parts = text.split('.', 1)
        main_part = parts[0].strip()
        subtitle = parts[1].strip() if len(parts) > 1 else ''

        main_part = re.sub(r'[^\w\s]', '', main_part)
        main_part = main_part.lower().strip()
        main_part = re.sub(r'[\s_]+', '', main_part)

        if subtitle:
            subtitle_words = re.sub(r'[^\w\s]', '', subtitle).split()
            subtitle_initials = ''.join(word[0].lower() for word in subtitle_words if word)
            return main_part + subtitle_initials
        return main_part
    else:
        text = re.sub(r'[^\w\s]', '', text)
        text = text.lower().strip()
        text = re.sub(r'[\s_]+', '', text)
        return text

def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Parse quiz pack PDF/DOCX files using Gemini API',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Single PDF file
  python parse_pdf_with_gemini.py pack.pdf

  # Folder with multiple files (auto-detected)
  python parse_pdf_with_gemini.py –õ–∞–≥–µ—Ä—å_–ë–ª–∏–∫_2024_–≠–ö/

  # Explicit folder mode
  python parse_pdf_with_gemini.py –õ–∞–≥–µ—Ä—å_–ë–ª–∏–∫_2024_–≠–ö/ --folder

  # Use plain text mode (faster)
  python parse_pdf_with_gemini.py pack.docx --text-mode
        """
    )
    parser.add_argument('input_path', help='PDF file, DOCX file, or folder path')
    parser.add_argument('--folder', action='store_true',
                       help='Treat input_path as folder and merge files')
    parser.add_argument('--single-shot', action='store_true',
                       help='Use single-shot parsing (faster, but may hit limits)')
    parser.add_argument('--text-mode', action='store_true',
                       help='Extract plain text and send to Gemini (faster, no file upload)')

    args = parser.parse_args()

    input_path = Path(args.input_path)
    chunked = not args.single_shot
    text_mode = args.text_mode
    is_merged = False
    merged_pdf_path = None
    file_path = None

    if args.folder or (input_path.exists() and input_path.is_dir()):
        print(f"Folder mode: processing {input_path}")
        print("-" * 60)

        pdf_files = sorted([f for f in input_path.glob('*.pdf')])
        docx_files = sorted([f for f in input_path.glob('*.docx') if not f.name.startswith('~$')])

        if not pdf_files and not docx_files:
            print(f"‚ùå Error: No PDF or DOCX files found in {input_path}")
            sys.exit(1)

        print(f"Found {len(pdf_files)} PDF file(s) and {len(docx_files)} DOCX file(s)")

        all_pdfs = []
        temp_converted_pdfs = []

        if pdf_files:
            all_pdfs.extend(pdf_files)
            print(f"  ‚úì {len(pdf_files)} PDF file(s) ready")

        if docx_files:
            print(f"  Converting {len(docx_files)} DOCX file(s) to PDF...")
            for i, docx_file in enumerate(docx_files, 1):
                print(f"    [{i}/{len(docx_files)}] Converting {docx_file.name}...")
                try:
                    converted_pdf = convert_docx_to_pdf(str(docx_file))
                    converted_pdf_path = Path(converted_pdf)
                    all_pdfs.append(converted_pdf_path)
                    temp_converted_pdfs.append(converted_pdf_path)
                    print(f"      ‚úì Created: {converted_pdf_path.name}")
                except Exception as e:
                    print(f"      ‚ö†Ô∏è  Failed to convert {docx_file.name}: {e}")
                    print(f"      Skipping this file...")

        if not all_pdfs:
            print(f"‚ùå Error: No valid PDF files to process")
            sys.exit(1)

        all_pdfs = sorted(all_pdfs, key=lambda p: p.name)

        merged_pdf_name = f"{input_path.name}_merged.pdf"
        merged_pdf_path = input_path / merged_pdf_name

        print(f"\nMerging {len(all_pdfs)} PDF file(s) into one...")
        merge_pdfs(all_pdfs, str(merged_pdf_path))
        print(f"‚úì Merged PDF created: {merged_pdf_path}")

        file_path = str(merged_pdf_path)
        is_merged = True
    elif input_path.exists() and input_path.is_file():
        file_path_str = str(input_path)

        if input_path.suffix.lower() == '.docx' and not text_mode:
            print("Converting DOCX to PDF...")
            file_path = convert_docx_to_pdf(file_path_str)
        else:
            file_path = file_path_str
    else:
        print(f"Error: Path not found: {input_path}")
        sys.exit(1)

    if is_merged:
        output_path = input_path / f"{input_path.name}_parsed.json"
    else:
        output_path = Path(file_path).parent / f"{Path(file_path).stem}_parsed.json"
    
    print(f"Input: {input_path}")
    print(f"Output JSON: {output_path}")
    print(f"Parsing mode: {'Chunked (theme-by-theme)' if chunked else 'Single-shot'}")
    print(f"Processing mode: {'Plain text' if text_mode else 'File upload'}")
    print(f"File format: {Path(file_path).suffix.upper()}")
    if is_merged:
        print("Note: Processing merged file (multiple –±–æ—ë–≤)")
    print("-" * 60)

    try:
        result = parse_pdf_with_gemini(file_path, chunked=chunked, text_mode=text_mode, is_merged=is_merged)

        save_json(result, str(output_path))
        print()

        validation_passed = validate_json_structure(result)
        
        if not validation_passed:
            print("\n‚ö†Ô∏è  WARNING: JSON validation failed!")
            print("The JSON structure is incomplete or missing 'form' fields.")
            print(f"‚ö†Ô∏è  JSON saved anyway to: {output_path}")
            print("You can fix it manually or re-run the parser.")
            sys.exit(1)

        print("-" * 60)
        print("‚úì Success! File parsed and saved to JSON")
        print(f"  Themes: {len(result['themes'])}")
        print(f"  Total questions: {sum(len(t['questions']) for t in result['themes'])}")
        if is_merged:
            print(f"  Merged DOCX saved: {file_path}")

        print()
        print("=" * 60)

        short_name = generate_short_name(input_path.name)

        package_name = result.get('package_name', '')

        command = f"python3 scripts/append_pack.py {short_name} {output_path}"
        if package_name:
            command += f' --name "{package_name}"'

        print("üìã Next step: Copy and run this command to add pack to database:")
        print()
        print(f"  {command}")
        print()
        print("=" * 60)

    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
